{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f165892",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import stanza\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pickle \n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from os.path import expanduser\n",
    "import json\n",
    "import functools\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3949f44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f9b3b13cd44334aacfaf2132f6bac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:46:15 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-08-23 11:46:15 INFO: Use device: gpu\n",
      "2022-08-23 11:46:15 INFO: Loading: tokenize\n",
      "2022-08-23 11:46:18 INFO: Loading: pos\n",
      "2022-08-23 11:46:18 INFO: Loading: lemma\n",
      "2022-08-23 11:46:18 INFO: Loading: depparse\n",
      "2022-08-23 11:46:18 INFO: Loading: sentiment\n",
      "2022-08-23 11:46:19 INFO: Loading: constituency\n",
      "2022-08-23 11:46:19 INFO: Loading: ner\n",
      "2022-08-23 11:46:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, home+'/Documents/detect_hedges/')\n",
    "\n",
    "from hedge_detection import isHedgedSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c1e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATA_DIR = home+'/Documents/spinos-project/data/' \n",
    "DATA_FN = 'spinos_user_history.pkl' \n",
    "with open(os.path.join(DATA_DIR, DATA_FN), \"rb\") as f:\n",
    "     df_hist = pickle.load(f)\n",
    "\n",
    "\n",
    "DATA_DIR = home+'/Documents/spinos-project/data/' \n",
    "DATA_FN = 'spinos_annotated.pkl'\n",
    "with open(os.path.join(DATA_DIR, DATA_FN), \"rb\") as f:\n",
    "     df = pickle.load(f)\n",
    "        \n",
    "        \n",
    "DATA_DIR = home+'/Documents/spinos-project/data/' \n",
    "DATA_FN = 'spinos_context.pkl'\n",
    "with open(os.path.join(DATA_DIR, DATA_FN), \"rb\") as f:\n",
    "     df_cont = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e528b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6527b872143434c8e1cdb7fa40f7a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:46:29 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-08-23 11:46:29 INFO: Use device: gpu\n",
      "2022-08-23 11:46:29 INFO: Loading: tokenize\n",
      "2022-08-23 11:46:29 INFO: Loading: pos\n",
      "2022-08-23 11:46:30 INFO: Loading: lemma\n",
      "2022-08-23 11:46:30 INFO: Loading: depparse\n",
      "2022-08-23 11:46:30 INFO: Loading: sentiment\n",
      "2022-08-23 11:46:30 INFO: Loading: constituency\n",
      "2022-08-23 11:46:31 INFO: Loading: ner\n",
      "2022-08-23 11:46:31 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from hedge_detection_c import Load_Lexicons, Detect_Hedges\n",
    "ll = Load_Lexicons()\n",
    "lexicons = ll.load()\n",
    "\n",
    "dh = Detect_Hedges(lexicons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e96a49a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c61eaeeb33547b287c513bf89e8d610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:46:33 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-08-23 11:46:33 INFO: Use device: gpu\n",
      "2022-08-23 11:46:33 INFO: Loading: tokenize\n",
      "2022-08-23 11:46:33 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n",
    "def hedge_percentage(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    list_hedges = []\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        list_hedges.append(isHedgedSentence(sentence.text))\n",
    "    \n",
    "    return sum(list_hedges)/len(list_hedges)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df41a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:50,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.444102654999995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "start = time.process_time()\n",
    "\n",
    "list_hedges = []\n",
    "for i, row in tqdm(df.head(100).iterrows()):\n",
    "    x = row['all_text']\n",
    "    try:\n",
    "        list_hedges.append(hedge_percentage(x))\n",
    "    except:\n",
    "        print(x)\n",
    "        list_hedges.append(np.nan)\n",
    "\n",
    "print(time.process_time() - start)\n",
    "\n",
    "# df['hedge_percentage'] = list_hedges\n",
    "# df['hedge_percentage'] = df['hedge_percentage'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3abf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:03,  1.25it/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "\n",
    "list_hedges = []\n",
    "for i, row in tqdm(df.head(100).iterrows()):\n",
    "    x = row['all_text']\n",
    "    try:\n",
    "        list_hedges.append(dh.hedge_percentage(x))\n",
    "    except:\n",
    "        print(x)\n",
    "        list_hedges.append(np.nan)\n",
    "\n",
    "print(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d022e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112a067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677b8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48552e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7210db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = home+'/Documents/spinos-project/data/' \n",
    "# DATA_FN = 'spinos_annotated.pkl'\n",
    "\n",
    "# df.to_pickle(DATA_DIR+DATA_FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fake] *",
   "language": "python",
   "name": "conda-env-fake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
