{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76ffa48",
   "metadata": {},
   "source": [
    "This is an implementation of the lexicon- and rule-based algorithm that detects hedges from these papers:\n",
    "\n",
    "    https://aclanthology.org/2020.lrec-1.380.pdf\n",
    "    \n",
    "    https://aclanthology.org/W18-1301.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc61a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10410108404642349b749c159a1dea71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 08:35:41 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-08-09 08:35:41 INFO: Use device: gpu\n",
      "2022-08-09 08:35:41 INFO: Loading: tokenize\n",
      "2022-08-09 08:35:44 INFO: Loading: pos\n",
      "2022-08-09 08:35:44 INFO: Loading: lemma\n",
      "2022-08-09 08:35:44 INFO: Loading: depparse\n",
      "2022-08-09 08:35:44 INFO: Loading: sentiment\n",
      "2022-08-09 08:35:44 INFO: Loading: constituency\n",
      "2022-08-09 08:35:45 INFO: Loading: ner\n",
      "2022-08-09 08:35:45 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a8326e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(A, B):\n",
    "    A = set(A)\n",
    "    B = set(B)\n",
    "    #Find intersection of two sets\n",
    "    nominator = A.intersection(B)\n",
    "\n",
    "    #Find union of two sets\n",
    "    denominator = A.union(B)\n",
    "\n",
    "    #Take the ratio of sizes\n",
    "    similarity = len(nominator)/len(denominator)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def txt2list(filename):\n",
    "    txt_file = open(filename, \"r\")\n",
    "    file_content = txt_file.read()\n",
    "\n",
    "    content_list = file_content.split(\"\\n\")\n",
    "    txt_file.close()\n",
    "\n",
    "    return content_list\n",
    "\n",
    "def isTrueHedgeTerm(t, doc_dicts, ids):\n",
    "    # rule-based algorithm\n",
    "    \n",
    "    true_hedge = False\n",
    "    hedge_terms = [\"feel\", \"suggest\", \"believe\", \"consider\", \"doubt\", \"guess\", \"hope\"]\n",
    "    \n",
    "    for id in ids:\n",
    "        if t == 'think':\n",
    "            if id+1 < len(doc_dicts):\n",
    "                if not (doc_dicts[id+1]['xpos'] =='IN'):\n",
    "                    true_hedge = True\n",
    "        if t == 'rather':\n",
    "            if id+1 < len(doc_dicts):\n",
    "                if not (doc_dicts[id+1]['text'] =='than'):\n",
    "                    true_hedge = True\n",
    "                    \n",
    "        if doc_dicts[id]['xpos'] == 'VBP':\n",
    "            is_verb = True\n",
    "        else:\n",
    "            is_verb = False\n",
    "        if doc_dicts[id]['deprel'] == 'root':\n",
    "            is_root = True\n",
    "        else:\n",
    "            is_root = False\n",
    "            \n",
    "\n",
    "        dependencies = {}\n",
    "        for word in doc_dicts:\n",
    "            if t == 'tend':\n",
    "                if (word['deprel'] == 'xcomp') & (doc_dicts[word['head']-1]['text'] == doc_dicts[id]['text']):\n",
    "                    true_hedge = True\n",
    "                    \n",
    "            if t == 'appear':\n",
    "                if ((word['deprel'] == 'xcomp') | (word['deprel'] == 'ccomp')) & (doc_dicts[word['head']-1]['text'] == doc_dicts[id]['text']):\n",
    "                    true_hedge = True\n",
    "            if t in hedge_terms:\n",
    "                if (is_root & is_verb & (word['deprel'] == 'nsubj')  & (doc_dicts[word['head']-1]['text'] == doc_dicts[id]['text'])):\n",
    "                    true_hedge = True\n",
    "            if t == 'assume':\n",
    "                if (word['deprel'] == 'ccomp') & (doc_dicts[word['head']-1]['text'] == doc_dicts[id]['text']):\n",
    "                        true_hedge = True                    \n",
    "\n",
    "            parent = str(doc_dicts[word['head']-1]['text'] if word['head'] > 0 else 'ROOT')\n",
    "            try:\n",
    "                dependencies[parent].setdefault(word['deprel'],[]).append(word['text']) \n",
    "\n",
    "            except KeyError:\n",
    "                dependencies[parent] = {}\n",
    "                dependencies[parent][word['deprel']] = [word['text']]\n",
    "\n",
    "        if t == 'suppose':\n",
    "            true_hedge = True\n",
    "            try:\n",
    "                if 'xcomp' in dependencies[doc_dicts[id]['text']].keys():\n",
    "                    xcomps = dependencies[doc_dicts[id]['text']]['xcomp']\n",
    "                    for xcomp in xcomps:\n",
    "                        try:\n",
    "                            true_hedge = (not 'to' in dependencies[xcomp]['mark'])\n",
    "                        except:\n",
    "                            true_hedge = True   \n",
    "\n",
    "            except:\n",
    "                if doc_dicts[id]['deprel'] == 'advmod':\n",
    "                    true_hedge = False\n",
    "                        \n",
    "        if t == 'likely':\n",
    "            true_hedge = True\n",
    "            if (doc_dicts[id]['deprel'] == 'amod') & (doc_dicts[doc_dicts[id]['head']-1]['xpos'] == 'NN'):\n",
    "                true_hedge = False\n",
    "\n",
    "        if t == 'should':\n",
    "            true_hedge = True\n",
    "            try:\n",
    "                root = dependencies['ROOT']['root'][0]                \n",
    "                if t in dependencies[root]['aux']:\n",
    "                    flatten_values = [element for sublist in dependencies[root].values() for element in sublist]\n",
    "                    true_hedge = not 'have' in flatten_values\n",
    "            except:\n",
    "                true_hedge = True \n",
    "                \n",
    "        if t == 'about':\n",
    "            true_hedge = True\n",
    "            if doc_dicts[id]['xpos'] == 'IN':\n",
    "                true_hedge = False\n",
    "                \n",
    "        if t == 'sure':\n",
    "            try:\n",
    "                flatten_values = [element for sublist in dependencies[doc_dicts[id]['text']].values() for element in sublist]\n",
    "                true_hedge = (\"not\" in flatten_values) | (\"n't\" in flatten_values)\n",
    "            except:\n",
    "                true_hedge = False\n",
    "                \n",
    "        if t == 'completely':\n",
    "            try:\n",
    "                if doc_dicts[id]['head'] > 0:\n",
    "                    \n",
    "                    head_t = doc_dicts[doc_dicts[id]['head']-1]['text']\n",
    "                flatten_values = [element for sublist in dependencies[head_t].values() for element in sublist]\n",
    "                true_hedge = (\"not\" in flatten_values) | (\"n't\" in flatten_values)\n",
    "            except:\n",
    "                true_hedge = False\n",
    "        \n",
    "#     try UnboundLocalError:\n",
    "    if true_hedge:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def isHedgedSentence(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    doc_dicts = doc.sentences[0].to_dict()\n",
    "    try:\n",
    "        P = [doc_dict['lemma'] for doc_dict in doc_dicts]\n",
    "    except KeyError:\n",
    "        P = []\n",
    "        for doc_dict in doc_dicts:\n",
    "            try:\n",
    "                P.append(doc_dict['lemma'])\n",
    "            except KeyError:\n",
    "                P.append(doc_dict['text'])\n",
    "    \n",
    "    status = False\n",
    "    \n",
    "    threshold = 0.8\n",
    "\n",
    "    for A in DM:\n",
    "        for B in P:\n",
    "            if 1 - jaccard_similarity(A, B) >= threshold:\n",
    "                status = True\n",
    "\n",
    "    for booster in B:\n",
    "        indices = [i for i, s in enumerate(sentence) if booster in s]\n",
    "        if len(indices)>0:\n",
    "            for idx in indices[1:]:\n",
    "                if (sentence[idx-1] == 'not') or (sentence[idx-1] == 'without'):\n",
    "                    status = True\n",
    "\n",
    "    \n",
    "    for hedge in HG:\n",
    "        indices = [i for i, s in enumerate(P) if hedge in s]\n",
    "        \n",
    "        if (hedge in sentence) and isTrueHedgeTerm(hedge, doc_dicts, indices):\n",
    "\n",
    "            status = True\n",
    "            break\n",
    "        else:\n",
    "            status = False\n",
    "            \n",
    "            \n",
    "    return status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbd44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DM = txt2list(\"resources/discourse_markers.txt\") # List of discourse markers\n",
    "HG1 = txt2list(\"resources/hedge_words.txt\") # List of hedge words\n",
    "HG2 = txt2list(\"resources/propositional_hedges.txt\")\n",
    "HG3 = txt2list(\"resources/relational_hedges.txt\")\n",
    "HG = list(set(HG1+HG2+HG3))\n",
    "B = txt2list(\"resources/booster_words.txt\") # List of booster words\n",
    "nlp = stanza.Pipeline('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a00bab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Ambiguous examples: \n",
    "# All answers must be s1: True, s2: False\n",
    "\n",
    "# s1 = \"We tend to never forget.\"\n",
    "# s2 = \"All political institutions tended toward despotism.\"\n",
    "# t = 'tend'\n",
    "\n",
    "# s1 = \"I hope that I'm on the right track.\"\n",
    "# s2 = \"I'm still living with it, but without hope that I would find anyone.\"\n",
    "# t = 'hope'\n",
    "\n",
    "# s1 = \"I think it's difficult to make generalizations about this kind of relationships.\"\n",
    "# s2 = \"Even if it's difficult, I always say, think about your children.\"\n",
    "# t = 'think'\n",
    "\n",
    "# s1 = \"I assume they were responsible for this.\"\n",
    "# s2 = \"They have assumed the role of parents and are doing their best to fulfill it.\"\n",
    "# t = \"assume\"\n",
    "\n",
    "# s1 = \"I suppose he was present during the discussion.\"\n",
    "# s2 = \"I could see that they were skewing the real truth, the one they are supposed to tell me.\"\n",
    "# t = \"suppose\"\n",
    "\n",
    "\n",
    "# s1 = \"They will likely visit us in the future.\"\n",
    "# s2 = \"He is a fine, likely man.\"\n",
    "# t = \"likely\"\n",
    "\n",
    "# s1 = \"That's precisely the message that should be sent to people who label others, isn't it?\"\n",
    "# s2 = \"They should have been more careful.\"\n",
    "# t = \"should\"\n",
    "\n",
    "# s1 = \"I never had the opportunity to go, but i know people who have gone and who came back rather depressed.\"\n",
    "# s2 = \"He would have protected his flock rather than shoot at them.\"\n",
    "# t = \"rather\"\n",
    "\n",
    "# s1 = \"There are about 10 million packages in transit right now.\" \n",
    "# s2 = \"We need to talk about Mark.\"\n",
    "# t = \"about\"\n",
    "\n",
    "# s1 = \"I am not sure.\"\n",
    "# s2 = \"He is sure she will turn up tomorrow.\"\n",
    "# t = \"sure\"\n",
    "\n",
    "# s1 = \"That isn't completely true.\" \n",
    "# s2 = \"I am completely sure you will win\"\n",
    "# t = \"completely\"\n",
    "\n",
    "# print dependencies\n",
    "# for word in doc_dicts:\n",
    "#   print (\"{:<15} | {:<10} | {:<15} \"\n",
    "#          .format(str(word['text']),str(word['deprel']), str(doc_dicts[word['head']-1]['text'] if word['head'] > 0 else 'ROOT')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(isHedgedSentence(s1))\n",
    "print(isHedgedSentence(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "865bfb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://old.reddit.com/r/brexit/comments/eo0e0b/i_dont_think_brexiters_hate_foreigners_i_think/fe79u02/) | root       | ROOT            \n",
      "\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "s1 = \"https://old.reddit.com/r/brexit/comments/eo0e0b/i_dont_think_brexiters_hate_foreigners_i_think/fe79u02/)\"\n",
    "t = \"think\"\n",
    "\n",
    "doc = nlp(s1)\n",
    "doc_dicts = doc.sentences[0].to_dict()\n",
    "P = [doc_dict['lemma'] for doc_dict in doc_dicts]\n",
    "indices = [i for i, s in enumerate(P) if t in s]\n",
    "\n",
    "id = indices[0]\n",
    "doc_dicts[id]\n",
    "\n",
    "\n",
    "s = doc_dicts\n",
    "ids = indices\n",
    "\n",
    "# isTrueHedgeTerm(t, s, ids)\n",
    "\n",
    "# print dependencies\n",
    "for word in doc_dicts:\n",
    "  print (\"{:<15} | {:<10} | {:<15} \"\n",
    "         .format(str(word['text']),str(word['deprel']), str(doc_dicts[word['head']-1]['text'] if word['head'] > 0 else 'ROOT')))\n",
    "print()\n",
    "print(isHedgedSentence(s1))\n",
    "print(isHedgedSentence(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6a84e51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://old.reddit.com/r/brexit/comments/eo0e0b/i_dont_think_brexiters_hate_foreigners_i_think/fe79u02/)']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2403b6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'text': 'https://old.reddit.com/r/brexit/comments/eo0e0b/i_dont_think_brexiters_hate_foreigners_i_think/fe79u02/)',\n",
       "  'lemma': 'https://old.reddit.com/r/brexit/comments/eo0e0b/i_dont_think_brexiters_hate_foreigners_i_think/fe79u02/)',\n",
       "  'upos': 'X',\n",
       "  'xpos': 'ADD',\n",
       "  'head': 0,\n",
       "  'deprel': 'root',\n",
       "  'start_char': 0,\n",
       "  'end_char': 104,\n",
       "  'ner': 'O',\n",
       "  'multi_ner': ('O',)}]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if hedge == 'about':\n",
    "        \n",
    "    elif hedge == 'likely':\n",
    "        \n",
    "    elif hedge == 'rather':\n",
    "        \n",
    "    elif hedge == 'assume':\n",
    "        \n",
    "    elif hedge == 'tend':\n",
    "        \n",
    "    elif hedge == 'appear':\n",
    "        \n",
    "    elif hedge == 'sure':\n",
    "        \n",
    "    elif hedge == 'completely':\n",
    "        \n",
    "    elif hedge == 'suppose':\n",
    "        \n",
    "    elif hedge == 'should':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a01c4d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'is', 'n’t', 'completely', '.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a4e67787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "if t == 'completely':\n",
    "    try:\n",
    "        if doc_dicts[id]['head'] > 0:\n",
    "            head_t = doc_dicts[doc_dicts[id]['head']-1]['text']\n",
    "        flatten_values = [element for sublist in dependencies[head_t].values() for element in sublist]\n",
    "        true_hedge = (\"not\" in flatten_values) | (\"n't\" in flatten_values)\n",
    "        print('a')\n",
    "    except:\n",
    "        true_hedge = False\n",
    "print(true_hedge)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fake]",
   "language": "python",
   "name": "conda-env-fake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
